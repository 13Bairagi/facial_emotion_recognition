{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset with Plain_Dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plain_Dataset(Dataset):\n",
    "    def __init__(self,csv_file,img_dir,datatype,transform):\n",
    "        '''\n",
    "        Documentation\n",
    "        NO OneHot encoding\n",
    "        '''\n",
    "        self.csv_file = pd.read_csv(csv_file)\n",
    "        self.lables = self.csv_file['emotion']\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.datatype = datatype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_file)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img = Image.open(self.img_dir+self.datatype+str(idx)+'.jpg')\n",
    "        lables = np.array(self.lables[idx])\n",
    "        lables = torch.from_numpy(lables).long()\n",
    "\n",
    "\n",
    "        if self.transform :\n",
    "            img = self.transform(img)\n",
    "\n",
    "\n",
    "        return img,lables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "dataset = Plain_Dataset(csv_file='Dataset/Kaggle/test.csv',img_dir = 'test/',datatype = 'test',transform = transformation)\n",
    "test_loader =  DataLoader(dataset,batch_size=64,num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgg = dataset.__getitem__(250)[0]\n",
    "lable = dataset.__getitem__(250)[1]\n",
    "\n",
    "print(lable)\n",
    "#(0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral)\n",
    "imgnumpy = imgg.numpy()\n",
    "imgt = imgnumpy.squeeze()\n",
    "plt.imshow(imgt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Emotion(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Documentation\n",
    "        '''\n",
    "        super(Deep_Emotion,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,3)\n",
    "        self.conv2 = nn.Conv2d(10,10,3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(10,10,3)\n",
    "        self.conv4 = nn.Conv2d(10,10,3)\n",
    "        self.pool4 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        #self.dropout = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(810,50)\n",
    "        self.fc2 = nn.Linear(50,7)\n",
    "\n",
    "    def forward(self,input):\n",
    "        out = self.conv1(input)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.pool2(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.pool4(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = F.dropout(out)\n",
    "        out = out.view(-1, 810) #####\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"model_noSTN-20-128-0.005.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total = []\n",
    "with torch.no_grad():\n",
    "    for data, lables in test_loader:\n",
    "        data, lables = data.cuda(), lables.cuda()\n",
    "        outputs = model(data)\n",
    "        pred = F.softmax(outputs)\n",
    "        classs = torch.argmax(pred,1)\n",
    "        wrong = torch.where(classs != lables,torch.tensor([1.]).cuda(),torch.tensor([0.]).cuda())\n",
    "        acc = 1- (torch.sum(wrong) / 64)\n",
    "        total.append(acc.item())\n",
    "    \n",
    "       # _, predicted = torch.max(outputs.data, 1)\n",
    "       # total += lables.size(0)\n",
    "       # correct += (predicted == lables).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * np.mean(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('Angry', 'Disgust', 'Fear', 'Happy',\n",
    "           'Sad', 'Surprise', 'Neutral')\n",
    "class_correct = list(0. for i in range(7))\n",
    "class_total = list(0. for i in range(7))\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(7):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "def load_img(path):\n",
    "    img = Image.open(path)\n",
    "    img = loader(img).float()\n",
    "    img = torch.autograd.Variable(img,requires_grad = True)\n",
    "    img = img.unsqueeze(0)\n",
    "    return img.cuda()\n",
    "\n",
    "def test_img(path,save_name):\n",
    "    #scale and convert to grayscale then save the image to import it with PIL.Image\n",
    "    img = cv2.imread(path,0)\n",
    "    img = cv2.resize(img,(48,48))\n",
    "    cv2.imwrite(save_name,img)\n",
    "    \n",
    "    #load saved image with PIL\n",
    "    PIL_img = load_img(path)\n",
    "    out = model(PIL_img)\n",
    "    pred = F.softmax(out)\n",
    "    classs = torch.argmax(pred,1)\n",
    "    wrong = torch.where(classs != 3,torch.tensor([1.]).cuda(),torch.tensor([0.]).cuda()) \n",
    "    classs = torch.argmax(pred,1)\n",
    "    prediction = classes[classs.item()]\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img('test2.jpg','test2.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the cascade\n",
    "face_cascade = cv2.CascadeClassifier('facedetection-master/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# To capture video from webcam. \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read the frame\n",
    "    _, img = cap.read()\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect the faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    # Draw the rectangle around each face\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        roi = cv2.cvtColor(roi,cv2.COLOR_BGR2GRAY)\n",
    "        roi = cv2.resize(roi,(48,48))\n",
    "        cv2.imwrite(\"roi.jpg\", roi)\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "    imgg = load_img(\"roi.jpg\")\n",
    "    out = model(imgg)\n",
    "    pred = F.softmax(out)\n",
    "    classs = torch.argmax(pred,1)\n",
    "    wrong = torch.where(classs != 3,torch.tensor([1.]).cuda(),torch.tensor([0.]).cuda())\n",
    "    classs = torch.argmax(pred,1)\n",
    "    prediction = classes[classs.item()]    \n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX   \n",
    "    org = (50, 50) \n",
    "    fontScale = 1\n",
    "    color = (255, 0, 0) \n",
    "    thickness = 2\n",
    "    img = cv2.putText(img, prediction, org, font,  \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('img', img)\n",
    "    # Stop if (Q) key is pressed\n",
    "    k = cv2.waitKey(30) \n",
    "    if k==ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "# Release the VideoCapture object\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to get the correct predictions for supervised learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_pred(preds,labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
