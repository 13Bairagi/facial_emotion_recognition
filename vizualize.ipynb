{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset with Plain_Dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plain_Dataset(Dataset):\n",
    "    def __init__(self,csv_file,img_dir,datatype,transform):\n",
    "        '''\n",
    "        Class inheret from Dataset pytorch class \n",
    "        the overrided methods used to collect the custom dataset (FER2013)\n",
    "        [Note] NO OneHot encoding is used with pytorch\n",
    "        '''\n",
    "        self.csv_file = pd.read_csv(csv_file)\n",
    "        self.lables = self.csv_file['emotion']\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.datatype = datatype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_file)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img = Image.open(self.img_dir+self.datatype+str(idx)+'.jpg')\n",
    "        lables = np.array(self.lables[idx])\n",
    "        lables = torch.from_numpy(lables).long()\n",
    "\n",
    "\n",
    "        if self.transform :\n",
    "            img = self.transform(img)\n",
    "\n",
    "\n",
    "        return img,lables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "dataset = Plain_Dataset(csv_file='Dataset/Kaggle/test.csv',img_dir = 'test/',datatype = 'test',transform = transformation)\n",
    "test_loader =  DataLoader(dataset,batch_size=64,num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgg = dataset.__getitem__(250)[0]\n",
    "lable = dataset.__getitem__(250)[1]\n",
    "\n",
    "print(lable)\n",
    "#(0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral)\n",
    "imgnumpy = imgg.numpy()\n",
    "imgt = imgnumpy.squeeze()\n",
    "plt.imshow(imgt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Emotion(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Documentation\n",
    "        '''\n",
    "        super(Deep_Emotion,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,3)\n",
    "        self.conv2 = nn.Conv2d(10,10,3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(10,10,3)\n",
    "        self.conv4 = nn.Conv2d(10,10,3)\n",
    "        self.pool4 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        #self.dropout = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(810,50)\n",
    "        self.fc2 = nn.Linear(50,7)\n",
    "\n",
    "    def forward(self,input):\n",
    "        out = self.conv1(input)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.pool2(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.pool4(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = F.dropout(out)\n",
    "        out = out.view(-1, 810) #####\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\omar sayed\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\serialization.py:401: UserWarning: Couldn't retrieve source code for container of type Deep_Emotion. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"model_noSTN-20-128-0.005.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\omar sayed\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 45 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total = []\n",
    "with torch.no_grad():\n",
    "    for data, lables in test_loader:\n",
    "        data, lables = data.cuda(), lables.cuda()\n",
    "        outputs = model(data)\n",
    "        pred = F.softmax(outputs)\n",
    "        classs = torch.argmax(pred,1)\n",
    "        wrong = torch.where(classs != lables,torch.tensor([1.]).cuda(),torch.tensor([0.]).cuda())\n",
    "        acc = 1- (torch.sum(wrong) / 64)\n",
    "        total.append(acc.item())\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (100 * np.mean(total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuarcy of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Angry : 42 %\n",
      "Accuracy of Disgust :  0 %\n",
      "Accuracy of  Fear :  7 %\n",
      "Accuracy of Happy : 73 %\n",
      "Accuracy of   Sad : 35 %\n",
      "Accuracy of Surprise : 47 %\n",
      "Accuracy of Neutral : 30 %\n"
     ]
    }
   ],
   "source": [
    "classes = ('Angry', 'Disgust', 'Fear', 'Happy','Sad', 'Surprise', 'Neutral')\n",
    "class_correct = list(0. for i in range(7))\n",
    "class_total = list(0. for i in range(7))\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(7):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function\n",
    "def get_correct_pred(preds,labels):\n",
    "    \"\"\"\n",
    "    preds : tensor with all predictions \n",
    "    labels : tensor with all correct labels \n",
    "    return the sum of all correct predictions then divide the number over the overall test examples to get the test accuracy \n",
    "    \"\"\"\n",
    "    return preds.cuda().argmax(dim=1).eq(labels.cuda()).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for plotting the confusion matrix \n",
    "@torch.no_grad() #grad tracking disabled \n",
    "def get_all_preds(model,loader):\n",
    "    \"\"\"\n",
    "    model : the pretrained network object\n",
    "    loader : data loader maybe (training, validation, test)\n",
    "    return tensor contain all the predictions of the dataloader passed \n",
    "    \"\"\"\n",
    "    all_preds = torch.tensor([]).cuda()\n",
    "    all_targets = torch.tensor([]).cuda()\n",
    "    for batch in loader :\n",
    "        imgs, labels = batch\n",
    "        labels = torch.tensor(labels,dtype=torch.float32)\n",
    "        \n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        \n",
    "        all_preds = torch.cat((all_preds,preds),dim=0)\n",
    "        all_targets = torch.cat((all_targets,labels),dim=0)\n",
    "    \n",
    "    all_targets = torch.tensor(all_targets,dtype=torch.long)    \n",
    "    return all_targets, all_preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\omar sayed\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "c:\\users\\omar sayed\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    all_targets, all_preds = get_all_preds(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3589, 7])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5218, -1.7244,  0.4855,  1.4294, -0.8410,  0.6596, -0.4255],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3589])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 4,  ..., 4, 4, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_correct_preds = get_correct_pred(all_preds,all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1590"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_correct_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuarcy  44.302034%\n"
     ]
    }
   ],
   "source": [
    "test_accuarcy = num_of_correct_preds / len(all_targets)\n",
    "print(\"Test Accuarcy {: 3f}%\".format(100*test_accuarcy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = all_preds.argmax(dim=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets = all_targets.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack the two previous tensors to bulid the confusion matrix \n",
    "stacked = torch.stack((all_targets,all_preds),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [1, 0],\n",
       "        [4, 2],\n",
       "        ...,\n",
       "        [4, 3],\n",
       "        [4, 0],\n",
       "        [4, 6]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3589, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the confusion tensor \n",
    "c_m = torch.zeros((7,7),dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop to fill the c_m tensor with the required numbers \n",
    "for p in stacked:\n",
    "    t, p = p.tolist()\n",
    "    c_m[t,p] = c_m[t,p] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[190,   0,  25,  67,  92,  18,  75],\n",
       "        [ 25,   0,   3,   6,  13,   3,   6],\n",
       "        [ 83,   1,  57,  94, 107,  62,  92],\n",
       "        [ 62,   0,  17, 633,  69,  21,  93],\n",
       "        [102,   0,  37, 128, 208,  32, 146],\n",
       "        [ 43,   0,  22,  46,  25, 247,  32],\n",
       "        [ 76,   0,  24,  95, 132,  25, 255]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Custom image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "def load_img(path):\n",
    "    img = Image.open(path)\n",
    "    img = loader(img).float()\n",
    "    img = torch.autograd.Variable(img,requires_grad = True)\n",
    "    img = img.unsqueeze(0)\n",
    "    return img.cuda()\n",
    "\n",
    "def test_img(path,save_name):\n",
    "    #scale and convert to grayscale then save the image to import it with PIL.Image\n",
    "    img = cv2.imread(path,0)\n",
    "    img = cv2.resize(img,(48,48))\n",
    "    cv2.imwrite(save_name,img)\n",
    "    \n",
    "    #load saved image with PIL\n",
    "    PIL_img = load_img(path)\n",
    "    out = model(PIL_img)\n",
    "    pred = F.softmax(out)\n",
    "    classs = torch.argmax(pred,1)\n",
    "    wrong = torch.where(classs != 3,torch.tensor([1.]).cuda(),torch.tensor([0.]).cuda())\n",
    "    classs = torch.argmax(pred,1)\n",
    "    prediction = classes[classs.item()]\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\omar sayed\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy\n"
     ]
    }
   ],
   "source": [
    "test_img('test2.jpg','test2.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\omar sayed\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-1375747c260b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Detect the faces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Draw the rectangle around each face\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the cascade\n",
    "face_cascade = cv2.CascadeClassifier('facedetection-master/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# To capture video from webcam. \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read the frame\n",
    "    _, img = cap.read()\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect the faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    # Draw the rectangle around each face\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        roi = cv2.cvtColor(roi,cv2.COLOR_BGR2GRAY)\n",
    "        roi = cv2.resize(roi,(48,48))\n",
    "        cv2.imwrite(\"roi.jpg\", roi)\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "    imgg = load_img(\"roi.jpg\")\n",
    "    out = model(imgg)\n",
    "    pred = F.softmax(out)\n",
    "    classs = torch.argmax(pred,1)\n",
    "    wrong = torch.where(classs != 3,torch.tensor([1.]).cuda(),torch.tensor([0.]).cuda())\n",
    "    classs = torch.argmax(pred,1)\n",
    "    prediction = classes[classs.item()]    \n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX   \n",
    "    org = (50, 50) \n",
    "    fontScale = 1\n",
    "    color = (255, 0, 0) \n",
    "    thickness = 2\n",
    "    img = cv2.putText(img, prediction, org, font,  \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('img', img)\n",
    "    # Stop if (Q) key is pressed\n",
    "    k = cv2.waitKey(30) \n",
    "    if k==ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "# Release the VideoCapture object\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
